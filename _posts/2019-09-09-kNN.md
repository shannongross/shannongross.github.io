---
published: false

layout: post
title: kNN notes
date: 2019-09-08
description:
fig-cation:
img: #books.jpg
tags: []
---
- TOC
{:toc}



## About kNN
The goal of the kNN algorithm is to use a number of nearby data points to make a prediction about a target point. Specifically, the number of nearby points used to generate the prediction is equal to k. For instance, if k=5 then we could look at the 5 nearest neighbors and take their mean to be the value of our target. Note that instead of the mean, it could also be the median or something else (depending on the loss function you choose). The algorithm can be used for classification or regression problems.
- kNN uses feature similarity to make predictions about new data points


The "nearest neighbors" are found by measuring the distance between the target point and the training points. The points with the shortest distances between them are ranked higher. The methods for measuring these distances vary, but common measures are:
  - using **Euclidean distance** for regression problems (continuous data)
  - using **Hamming distance** for classification problems (categorical data)

<!-- For those that are not familiar, the Hamming distance sums the number of characters that are different. This sum is the distance between two variables, so when variables have a small hamming distance they are closer together/ranked higher. -->

#About the *k* in kNN
There is no formal method for determining how many neighbors the algorithm should use to make a prediction. Typically, the process works like this:
  - Randomly pick a k-value to start with
  - Check the error value in its prediction
  - Raise the k-value to reduce the amount of error, then stop

While a higher k-value can increase the algorithm's performance, it also greatly increases the chance that you'll overfit to the training data.




## kNN Steps
1. Retrieve data
  - Specify k
3. iterate through training data:
  - find (Euclidian) distance between test data and each row of training data
  - sort distance values in ascending order
  - get top k rows from the sorted array
  - determine the most common class of these rows
  - return the predicted class  

Let's start by importing the python packages we'll be using:

{% highlight ruby %}
#standard imports
import pandas as pd
import plotly.express as px
import numpy as np
import matplotlib.pyplot as plt

#sklearn imports
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

#some styling options
%matplotlib inline
matplotlib.style.use('ggplot')
pd.options.display.max_columns = 99
{% endhighlight %}

Now, retrieve the data and do any necessary cleaning. Here, I'm using the World Development Indicator dataset from Kaggle, which you can download [here](www.google.com). In the code below, I merge the "Country.csv" data with the "Indicators.csv" data to obtain a dataframe containing information about the income, life expectancy, fertility rate, etc. per country. Because the full WDI dataset is very large, I only use a limited number of features and only look at the year 2012 to keep the following example simple.

{% highlight ruby %}
#Retrieve the Indicators.csv and Country.csv
df_indicators = pd.read_csv('world-development-indicators/Indicators.csv')
df_country = pd.read_csv('world-development-indicators/Country.csv')
#Obtain a small subset of the data
df_country = df_country[['ShortName','Region','IncomeGroup','SystemOfTrade']]
df_indicators = df_indicators[['CountryName', 'IndicatorName','Year','Value']]

# Get subset of interesting indicators
indicators_of_interest = [
     'Life expectancy at birth, total (years)',
     'Fertility rate, total (births per woman)',
     'Adjusted net national income per capita (current US$)',
     'Adjusted savings: education expenditure (current US$)',
     'Population, ages 0-14 (% of total)']

df_indicators_subset = df_indicators[df_indicators['IndicatorName'].isin(indicators_of_interest)]

# Get countries only
not_countries = ['Arab World', 'Caribbean small states',
       'Central Europe and the Baltics',
       'East Asia & Pacific (all income levels)',
       'East Asia & Pacific (developing only)', 'Euro area',
       'Europe & Central Asia (all income levels)',
       'Europe & Central Asia (developing only)', 'European Union',
       'Fragile and conflict affected situations',
       'Heavily indebted poor countries (HIPC)', 'High income',
       'High income: nonOECD', 'High income: OECD',
       'Latin America & Caribbean (all income levels)',
       'Latin America & Caribbean (developing only)',
       'Least developed countries: UN classification',
       'Low & middle income', 'Low income', 'Lower middle income',
       'Middle East & North Africa (all income levels)',
       'Middle East & North Africa (developing only)', 'Middle income',
       'North America', 'OECD members', 'Other small states',
       'Pacific island small states', 'Small states', 'South Asia',
       'Sub-Saharan Africa (all income levels)',
       'Sub-Saharan Africa (developing only)', 'Upper middle income']

df = df_indicators_subset[~df_indicators_subset['CountryName'].isin(not_countries)]

#only look at one year for simplicity
df = df[df.Year==2010]

#pivot
df = df.pivot(index='CountryName', columns='IndicatorName', values='Value')
df = pd.DataFrame(df.to_records())

#merge and clean
df = df.merge(df_country, left_on='CountryName', right_on='ShortName')
df = df.drop(['ShortName'],axis=1)
df = df.dropna(how='any')
df = df[['Adjusted net national income per capita (current US$)',
         'Adjusted savings: education expenditure (current US$)',
         'Fertility rate, total (births per woman)',
         'Life expectancy at birth, total (years)',
         'Population, ages 0-14 (% of total)',
        'IncomeGroup']]
df.info()
{% endhighlight %}

Note that I dropped the countries that had missing values using `.dropna()`. An alternative might have been to try and impute the missing values or another strategy to preserve these rows.

## Examine Data
Now that we have a reasonably clean dataset, we can create some initial visualizations to begin understanding what the data look like.

{% highlight ruby %}
#Scatterplot
fig = px.scatter(df,
                 x='Fertility rate, total (births per woman)',
                 y='Life expectancy at birth, total (years)',color='IncomeGroup')
fig.show()
{% endhighlight %}

<br>
![kNN simple scatter plot python](../assets/img/knn_plot1.jpg)
<br>


# Define a function to use kNN
The following function takes four inputs:
- the **target variable**, or name of the column that we are trying to predict
- a **list of features**, or names of the columns that we are using to help predict the target variable
- the name of the full **dataframe** containing this information
- the **k-value**, which is the number of nearby neighbors that will be looked at in order to predict the value of the target variable

{% highlight ruby %}
def find_kNN(feat_list, target, data, k_value):
    """
    feat_list: list of column names (features)
    target: label/output/thing to be predicted
    data: dataframe
    k_value: how many neighbors to look at

    """
    # preprocess
    X_in = data[feat_list].values
    y_in = data[[target]].values  

    #normalize/scale features
    scaler = StandardScaler()
    scaler.fit(X_in) # Fit the object to all the data except the Target Class
    scaled_features = scaler.transform(X_in)
    features_df = pd.DataFrame(scaled_features, columns=feat_list)

    #normalized x and y
    X = features_df[feat_list].values
    y = data[[target]].values

    #Now, split the normalized data into training and testing
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.20, random_state=13)

    # Create KNN instance and fit it
    classifier = KNeighborsClassifier(n_neighbors=k_value)
    classifier.fit(X_train, y_train)

    #make predictions
    y_pred = classifier.predict(X_test)

    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
{% endhighlight %}

## References
Complete guide:
https://ashutoshtripathi.com/2019/08/05/a-complete-guide-to-k-nearest-neighbors-algorithm-knn-using-python/


#EXTRA STUFF BELOW
### "Non-parametric" statistics
- not based solely on the mean and variance
- a function on a sample
- the data is not required to fit a normal distribution
- distribution-free (don't assume that your data follow a specific distribution)
- use this when your data doesn't meet the assumptions of the parametric test (that the data is normally distributed)
- use when your data is better represented by the median (instead of the mean)
- may be the only way to analyze data that are ordinal, ranked, intervals, have outliers, multiple peaks, a shift, or are measured imprecisely
- better for when data does not fit a well understood shape
- typically, the results of non-parametric results are less powerful. That's because they have to be generalized to work for many different types of data
- when you have ordinal or interval data, non-parametric statistics must be used
- you need non-parametric techniques in machine learning when your data doesn't fit the normal distribution

### "Parametric" statistics
- we have already identified the distribution and know its parameters
- most people who do statistics are more familiar with this
- use for means
- in general, only works on continuous data (can't do ranked or ordinal data). results can be swayed drastically by outliers
- assume that the data points are taken from a given probability distribution

## Classification vs Regression
- in the real world, classification is used more often than regression. That's because most business problems are about making a decision.


##disadvantage
- for high-dimensional data, the model is not really human-readable.
